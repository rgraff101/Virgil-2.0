## Reagan Graff
### This week:
- Got both motors working for the lidar program.
- Hard coded encoder distances to first bucket
- Program switch mode between encoder to reading lidar

### Goals:
- Research and train AI camera for determining color of bucket
- Program cases for buckets depending on color
- Brainstorm how the robot is going to navigate around bucket


## Kajsa Pruner
### This week:
- Got both motors spinning with the RPLidar program
- Did research on the AI Camera and how we are going to incorporate it and train it
- Did research on ROS2 if we realize in the future that we might need this. Seems like a good option if needed, because it would lead to flexibility switching between sensors and avoid one single large codebase. However, if our team is not comfortable with ROS2 it could lead to problems and setup time is a concern.
- When implementing the AI camera we will need to use YOLOv8n to train it to see the buckets and take pictures of the buckets in different lightnings etc. I found this tutorial that can be helpful: https://wiki.seeedstudio.com/tutorial_of_ai_kit_with_raspberrypi5_about_yolov8n_object_detection/?utm_source=chatgpt.com , along with this video that also can help us: https://youtu.be/7mh77Nq51Ho

### Goals:
- Incorporate AI camera and see if we are able to hardcode so that the sensors can switch inbetween each other for each step along the maze.


## Ethan Durham
### This week:

### Goals:
